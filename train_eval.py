# -*- coding: utf-8 -*-
"""train_eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mv5tbGCQAJfeap-gIMSBwj3J8_v0mwN2

### Install and setup
"""

USING_COLAB = False

if USING_COLAB:
    """!pip install transformers[sentencepiece]~=4.33.0 -qq
    !pip install datasets~=2.14.0 -qq
    !pip install accelerate~=0.23.0 -qq
    # test mlflow for logging
    !pip install mlflow~=2.7.0 -qq
    !pip install mdutils~=1.6.0 -qq
    !pip install scikit-learn~=1.2.0 -qq"""

## load packages
import transformers
import torch
from datasets import ClassLabel

import pandas as pd
import numpy as np
import os
from datasets import load_dataset
import re
import time
import random
import tqdm

from torch.utils.data import DataLoader
from datasets import load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets, list_metrics
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import TrainingArguments, Trainer

from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, accuracy_score, classification_report

import gc
from accelerate.utils import release_memory

import mlflow
import json
from datetime import datetime

from mdutils import MdUtils

## set global seed for reproducibility and against seed hacking
SEED_GLOBAL = 42
np.random.seed(SEED_GLOBAL)

## for tests in Colab
DATE = 20230928

if USING_COLAB:
    # info on the GPU you are using
    #!nvidia-smi
    # info on available ram
    from psutil import virtual_memory
    ram_gb = virtual_memory().total / 1e9
    print('\n\nYour runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if USING_COLAB:
    ## connect to google drive
    from google.colab import drive
    drive.mount('/content/drive', force_remount=False)

    #set wd
    print(os.getcwd())
    os.chdir("/content/drive/My Drive/PhD/zero-shot-models")

print(os.getcwd())

# local config.py file with tokens
import config

"""### Load data"""

# load from hub
dataset_train = load_dataset("MoritzLaurer/dataset_train_nli", token=config.HF_ACCESS_TOKEN)["train"]
dataset_test_concat_nli = load_dataset("MoritzLaurer/dataset_test_concat_nli", token=config.HF_ACCESS_TOKEN)["train"]
dataset_test_disaggregated = load_dataset("MoritzLaurer/dataset_test_disaggregated_nli", token=config.HF_ACCESS_TOKEN)

"""### Tokenize, train eval"""

### Load model and tokenizer

model_name = "microsoft/deberta-v3-base"
max_length = 512

## load model and tokenizer
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# label2id mapping
label2id = {"true": 0, "not_true": 1}  #{"entailment": 0, "neutral": 1, "contradiction": 2}
id2label = {0: "true", 1: "not_true"}  #{0: "entailment", 1: "neutral", 2: "contradiction"}

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=max_length)  # model_max_length=512
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, label2id=label2id, id2label=id2label
).to(device)

"""#### Tokenize"""

### tokenization
# Dynamic padding HF course: https://huggingface.co/course/chapter3/2?fw=pt

# without padding="max_length" & max_length=512, it should do dynamic padding.
def tokenize_func(examples):
    return tokenizer(examples["text"], examples["hypothesis"], truncation=True)  # max_length=512,  padding=True

# training on:
encoded_dataset_train = dataset_train.map(tokenize_func, batched=True)
print(len(encoded_dataset_train))
# testing on:
encoded_dataset_test = dataset_test_concat_nli.map(tokenize_func, batched=True)
print(len(encoded_dataset_test))
# testing on individual datasets:
encoded_dataset_test_disaggregated = dataset_test_disaggregated.map(tokenize_func, batched=True)

# remove columns the library does not expect
encoded_dataset_train = encoded_dataset_train.remove_columns(["hypothesis", "text"])
encoded_dataset_test = encoded_dataset_test.remove_columns(["hypothesis", "text"])

"""#### Training"""

# release memory: https://huggingface.co/blog/optimize-llm

def flush():
  gc.collect()
  torch.cuda.empty_cache()
  torch.cuda.reset_peak_memory_stats()

def compute_metrics_standard(eval_pred, label_text_alphabetical=None):
    labels = eval_pred.label_ids
    pred_logits = eval_pred.predictions
    preds_max = np.argmax(pred_logits, axis=1)  # argmax on each row (axis=1) in the tensor

    # metrics
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds_max, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html
    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds_max, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html
    acc_balanced = balanced_accuracy_score(labels, preds_max)
    acc_not_balanced = accuracy_score(labels, preds_max)

    metrics = {'f1_macro': f1_macro,
            'f1_micro': f1_micro,
            'accuracy_balanced': acc_balanced,
            'accuracy': acc_not_balanced,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'precision_micro': precision_micro,
            'recall_micro': recall_micro,
            #'label_gold_raw': labels,
            #'label_predicted_raw': preds_max
            }
    print("Aggregate metrics: ", {key: metrics[key] for key in metrics if key not in ["label_gold_raw", "label_predicted_raw"]} )  # print metrics but without label lists
    print("Detailed metrics: ", classification_report(
        labels, preds_max, labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),
        target_names=label_text_alphabetical, sample_weight=None,
        digits=2, output_dict=True, zero_division='warn'),
    "\n")

    return metrics


def compute_metrics_nli_binary(eval_pred, label_text_alphabetical=None):
    predictions, labels = eval_pred

    # split in chunks with predictions for each hypothesis for one unique premise
    def chunks(lst, n):  # Yield successive n-sized chunks from lst. https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    # for each chunk/premise, select the most likely hypothesis, either via raw logits, or softmax
    prediction_chunks_lst = list(chunks(predictions, len(set(label_text_alphabetical)) ))  # len(LABEL_TEXT_ALPHABETICAL)
    hypo_position_highest_prob = []
    for i, chunk in enumerate(prediction_chunks_lst):
        # only accesses the first column of the array, i.e. the entailment prediction logit of all hypos and takes the highest one
        hypo_position_highest_prob.append(np.argmax(chunk[:, 0]))

    label_chunks_lst = list(chunks(labels, len(set(label_text_alphabetical)) ))
    label_position_gold = []
    for chunk in label_chunks_lst:
        label_position_gold.append(np.argmin(chunk))  # argmin to detect the position of the 0 among the 1s

    # for inspection
    print("Highest probability prediction per premise: ", hypo_position_highest_prob)
    print("Correct label per premise: ", label_position_gold)

    ## metrics
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(label_position_gold, hypo_position_highest_prob, average='macro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html
    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(label_position_gold, hypo_position_highest_prob, average='micro')  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html
    acc_balanced = balanced_accuracy_score(label_position_gold, hypo_position_highest_prob)
    acc_not_balanced = accuracy_score(label_position_gold, hypo_position_highest_prob)
    metrics = {'f1_macro': f1_macro,
               'f1_micro': f1_micro,
               'accuracy_balanced': acc_balanced,
               'accuracy': acc_not_balanced,
               'precision_macro': precision_macro,
               'recall_macro': recall_macro,
               'precision_micro': precision_micro,
               'recall_micro': recall_micro,
               #'label_gold_raw': label_position_gold,
               #'label_predicted_raw': hypo_position_highest_prob
               }
    print("Aggregate metrics: ", {
        key: metrics[key] for key in metrics
        if key not in ["label_gold_raw", "label_predicted_raw"]
    })
    print("Detailed metrics: ", classification_report(
        label_position_gold,
        hypo_position_highest_prob,
        labels=np.sort(pd.factorize(label_text_alphabetical, sort=True)[0]),
        target_names=label_text_alphabetical,
        sample_weight=None, digits=2, output_dict=True,
        zero_division='warn'),
    "\n")

    return metrics

training_directory = f'./results/{model_name.split("/")[-1]}-zeroshot-{DATE}'

## test logging with mlflow
# https://mlflow.org/docs/latest/python_api/mlflow.transformers.html
# https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/mlflow/MLflow%20and%20Transformers.ipynb
# https://huggingface.co/docs/transformers/v4.33.3/en/main_classes/callback#transformers.integrations.MLflowCallback
#mlflow.create_experiment(name='your_experiment_name', artifact_location=f'{training_directory}/logs')
now = datetime.now().strftime("%Y-%m-%d-%H-%M")
run_name = f"mlflow-{model_name.split('/')[-1]}-{now}"
os.environ["MLFLOW_EXPERIMENT_NAME"] = f"mlflow-{model_name.split('/')[-1]}-zeroshot-{DATE}"
mlflow_tags = {
    "train_data": np.unique(dataset_train["task_name"]).tolist(),
    #"train_data_nli": np.unique(dataset_train_nli["task_name"]).tolist(),
    #"train_data_not_nli": np.unique(dataset_train_not_nli["task_name"]).tolist()
}
os.environ['MLFLOW_TAGS'] = json.dumps(mlflow_tags)
#os.environ["MLFLOW_FLATTEN_PARAMS"] = "1"
# https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.autolog
mlflow.autolog(log_datasets=False, log_models=False, silent=False)


fp16_bool = True if torch.cuda.is_available() else False
if "mDeBERTa" in model_name: fp16_bool = False  # mDeBERTa does not support FP16 yet

# https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments
eval_batch = 64

train_args = TrainingArguments(
    output_dir=training_directory,
    logging_dir=f'{training_directory}/logs',
    #deepspeed="ds_config_zero3.json",  # if using deepspeed
    lr_scheduler_type= "linear",
    group_by_length=False,  # can increase speed with dynamic padding, by grouping similar length texts https://huggingface.co/transformers/main_classes/trainer.html
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=eval_batch,
    gradient_accumulation_steps=2,  # (!adapt/halve batch size accordingly). accumulates gradients over X steps, only then backward/update. decreases memory usage, but also slightly speed
    #eval_accumulation_steps=2,
    num_train_epochs=2,
    #max_steps=400,
    #warmup_steps=0,  # 1000,
    warmup_ratio=0.06,  #0.1, 0.06
    weight_decay=0.01,  #0.1,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=fp16_bool,   # ! only makes sense at batch-size > 8. loads two copies of model weights, which creates overhead. https://huggingface.co/transformers/performance.html?#fp16
    fp16_full_eval=fp16_bool,
    evaluation_strategy="epoch",
    seed=SEED_GLOBAL,
    #eval_steps=300  # evaluate after n steps if evaluation_strategy!='steps'. defaults to logging_steps
    save_strategy="epoch",  # options: "no"/"steps"/"epoch"
    #save_steps=1_000_000,              # Number of updates steps before two checkpoint saves.
    save_total_limit=3,             # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir
    #logging_strategy="epoch",
    report_to="all",  # "all"
    run_name=run_name,
    #push_to_hub=True,
    #push_to_hub_model_id="test97531", #f"{model_name}-finetuned-{task}",
    #hub_token="XXX",  # for pushing to hub  # https://discuss.huggingface.co/t/where-to-put-use-auth-token-in-the-code-if-you-cant-run-hugginface-cli-login-command/11701/2
)

trainer = Trainer(
    model=model,
    #model_init=model_init,
    tokenizer=tokenizer,
    args=train_args,
    train_dataset=encoded_dataset_train.shard(index=1, num_shards=900),  # https://huggingface.co/docs/datasets/processing.html#sharding-the-dataset-shard
    eval_dataset=encoded_dataset_test.shard(index=1, num_shards=90),
    compute_metrics=lambda x: compute_metrics_standard(x, label_text_alphabetical=["true", "not_true"])  #compute_metrics,
    #data_collator=data_collator,  # for weighted sampling per dataset; for dynamic padding probably not necessary because done by default  https://huggingface.co/course/chapter3/3?fw=pt
)

if device == "cuda":
    # free memory
    flush()
    release_memory(model)
    #del (model, trainer)

# train
trainer.train()

"""#### Evaluation"""

# load specific model for evaluation
#model = AutoModelForSequenceClassification.from_pretrained('./results/nli-few-shot/all-nli-3c/DeBERTa-v3-mnli-fever-anli-v1',   # nli_effect/distilroberta-paraphrase-mnli-fever-anli-v1
#                                                           label2id=label2id, id2label=id2label).to(device)

# free memory
if device == "cuda":
    flush()
    release_memory(model)


result_dic = {}
for key_task_name, value_dataset in tqdm.notebook.tqdm(encoded_dataset_test_disaggregated.items(), desc="Iterations over testsets"):
    if key_task_name in dataset_test_disaggregated.keys():
        trainer.compute_metrics = lambda x: compute_metrics_standard(x, label_text_alphabetical=["true", "not_true"])
        result = trainer.evaluate(eval_dataset=encoded_dataset_test_disaggregated[key_task_name])
    elif any(long_dataset in key_task_name for long_dataset in ["anthropic", "banking77", "massive", "empathetic"]):
        # ! anthropic hypos not in task_hypotheses !
        # handle these (very long) datasets later
        continue
    else:
        label_text_alphabetical = np.sort(np.unique(value_dataset["label_text"])).tolist()
        trainer.compute_metrics = lambda x: compute_metrics_nli_binary(x, label_text_alphabetical=label_text_alphabetical)
        result = trainer.evaluate(eval_dataset=encoded_dataset_test_disaggregated[key_task_name])

    result_dic.update({key_task_name: result})
    print(f"Result for task {key_task_name}: ", result, "\n")

print("\n\nOverall results: ", result_dic)

# add disaggregated metrics to mlflow tracking
run_object = mlflow.search_runs(filter_string=f"attributes.run_name = '{run_name}'")
run_id = run_object["run_id"][0]

#with mlflow.start_run(run_id=run_id):
result_dic_unnested = {f"{outer_key}_{inner_key}": value for outer_key, inner_dict in result_dic.items() for inner_key, value in inner_dict.items()}
mlflow.log_metrics(result_dic_unnested)
mlflow.end_run()

"""## Create Model Card """

## testing automatic creation of .md file
# https://mdutils.readthedocs.io/en/latest/mdutils.html#subpackages
mdFile = MdUtils(file_name=f'README-{model_name.split("/")[-1]}-{DATE}', title='Model Card')

row_dataset_names = list(result_dic.keys())
row_metrics = [str(round(value["eval_accuracy"], 3)) for key, value in result_dic.items()]
row_samp_per_sec = [str(round(value["eval_samples_per_second"], 0)) for key, value in result_dic.items()]

table_lst = ["Datasets"] + row_dataset_names + ["Accuracy"] + row_metrics + [f"Inference text/sec (A100, batch={eval_batch})"] + row_samp_per_sec

# create markdown table with results
#mdFile.new_line()
results_table_me = mdFile.new_table(columns=len(list(result_dic.keys()))+1, rows=3, text=table_lst, text_align='center')
print(results_table_me)

# write results_table_me to training directors
path_main = os.getcwd()
os.chdir(training_directory)
mdFile.create_md_file()
os.chdir(path_main)

upload_to_hub = True

if upload_to_hub:
    # push directly via trainer to hub
    #trainer.push_to_hub()  # does not work for some reason. wheel spins but nothing happens.

    ## save best model to disk
    model_path = f"{training_directory}/best-{model_name.split('/')[-1]}-{DATE}"

    trainer.save_model(output_dir=model_path)

    print(os.getcwd())
    model = AutoModelForSequenceClassification.from_pretrained(model_path, torch_dtype=torch.float16)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, model_max_length=512)

    ## Push to hub
    #!sudo apt-get install git-lfs
    #!huggingface-cli login
    # unnecessary if token provided below

    # https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.push_to_hub
    # repo_path_or_name=f'{model_name.split("/")[-1]}-{"-".join(NLI_DATASETS_TO_USE)}',
    model.push_to_hub(repo_id=f'MoritzLaurer/{model_name.split("/")[-1]}-zeroshot-v1', use_temp_dir=True, private=True, use_auth_token=config.HF_ACCESS_TOKEN)
    tokenizer.push_to_hub(repo_id=f'MoritzLaurer/{model_name.split("/")[-1]}-zeroshot-v1', use_temp_dir=True, private=True, use_auth_token=config.HF_ACCESS_TOKEN)

"""### Optional: inspect MLflow traces in colab"""

if USING_COLAB:
    assert 1 == 2, "Block following code from executing when running entire notebook"

if USING_COLAB:
    ## inspect mlflow logs directly in colab with ngrok and terminal
    #!pip install "pyngrok~=5.2.1" -qqq

    import getpass
    from pyngrok import ngrok, conf

    print("Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth")
    #conf.get_default().auth_token = getpass.getpass()
    conf.get_default().auth_token = config.NGROK_ACCESS_TOKEN
    # if the above does not work, try:
    #ngrok.set_auth_token("<INSER_YOUR_NGROK_AUTHTOKEN>")

    print("In terminal, set working directory: cd", os.getcwd())
    print("then run command: mlflow ui")

if USING_COLAB:
    # disconnect all existing tunnels to avoid issues when rerunning cells
    [ngrok.disconnect(tunnel.public_url) for tunnel in ngrok.get_tunnels()]

    # create the public link
    ngrok_tunnel = ngrok.connect(5000)
    print("You can now access the Argilla localhost with the public link below. (It should look something like 'http://X03b-34-XXX-237-25.ngrok.io')\n")
    print(f"Your ngrok public link: {ngrok_tunnel}\n")
    print("After clicking on the link, there will be a warning, which you can ignore")